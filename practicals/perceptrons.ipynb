{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfhXV-UIQLJp"
      },
      "source": [
        "## Perceptrons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4zPF88nQLJq"
      },
      "source": [
        "*(Credit: Leon Derczynski, IT University of Copenhagen)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS2hWUPUQLJr"
      },
      "source": [
        "Let's build a little perceptron! It'll be on its own, which means it can only really do linearly separable problems. But that's OK; it'll try as hard as it can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIUdA4TdQLJs"
      },
      "source": [
        "First, we'll set our version of Python so other coders (and shell interpreters) can see what we're doing, and import two handy things: a random numbers module; and some extensions that help with many different kinds of numerical math. You might even say mathS, in fact. Together they're called numpy, pronounced Numb Pie. We will also add a module for plotting our results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcxU49b9QLJs"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from random import choice                 # we will use this to choose random example when training\n",
        "from numpy import array, dot              # numpy will help us when manipulating vectors\n",
        "import matplotlib.pyplot as plt           # to plot our results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngDYQbzLQLJt"
      },
      "source": [
        "Next, we'll define our training data. The format is thus, for each example:\n",
        "* an array containng the two input features, often together called *X*, followed with a bias value, which will be 1\n",
        "* the output label, *y*\n",
        "\n",
        "In our first case, we will model a boolean function, AND. In the AND function, if both of the two inputs are 1 (true), then the output will also be 1 (true):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92uHmN6KQLJu"
      },
      "outputs": [],
      "source": [
        "training_data = [\n",
        "\t(array([0,0,1]), 0),\n",
        "\t(array([0,1,1]), 0),\n",
        "\t(array([1,0,1]), 0),\n",
        "\t(array([1,1,1]), 1),\n",
        "\t]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNxg4UTfQLJv"
      },
      "source": [
        "Next, we'll set up our activation function. This is used to decide whether the output of our perceptron should be 1 or 0. We will use a very simple unit step function. If the result of our perceptron is less than zero, we will output zero. Otherwise, we will output 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsgTSvHdQLJv"
      },
      "outputs": [],
      "source": [
        "# Activation function. If the input is less than zero,\n",
        "# returns zero, otherwise return 1. This is a unit step\n",
        "# activation function.\n",
        "def activation(x):\n",
        "  if x < 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVGqECwlQLJw"
      },
      "source": [
        "Finally come the parameters that define how our perceptron behaves. We refer to these as hyperparameters, to distinguish them from the weights, which are sometimes called the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvqInIfrQLJw"
      },
      "outputs": [],
      "source": [
        "epochs = 100              # the number of training iterations, which we call epochs\n",
        "learning_rate = 0.8       # the learning rate - scales how much we update on each iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will make a little list where we'll keep track of how well training has gone. Oh, and while we're at it, let's initialise the weights too."
      ],
      "metadata": {
        "id": "SvSbDx6SUvp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "errors = []               # an array to store the errors in\n",
        "weights = random.rand(3)  # an array of initial weights"
      ],
      "metadata": {
        "id": "77pw5i9CU229"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aiVTcsyQLJw"
      },
      "source": [
        "So next, we have the training process. At each epoch (iteration, step), we randomly select a training example. With that example, we work out the [dot product](https://www.mathsisfun.com/algebra/vectors-dot-product.html) of the features and our current weights. This gives us the activation potential - how much our neuron is trying to fire.\n",
        "\n",
        "Next, we put this through our activation function to see what our neuron really does, and compare that to what the answer should be, for this example. The difference is our error; how far wrong were we? We'll store that error so we can view them later.\n",
        "\n",
        "In the mean time, we'll update our weights, so they become closer to where they should have been. i.e. we try to reduce the error to zero. The learning rate scales how big that update is. Here's the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4KQ7Qc5QLJw"
      },
      "outputs": [],
      "source": [
        "# Do this for every epoch\n",
        "for i in range(epochs):\n",
        "\n",
        "\t# Choose a random training example, setting the variabel x to\n",
        "\t# be the input vector, and the variable expected to be the expected output\n",
        "\tx, expected = choice(training_data)\n",
        "\n",
        "  # Find the dot product of the current weights and the inputs\n",
        "\tresult = dot(weights, x)\n",
        "\n",
        "  # Run the result through out activation function to get the output\n",
        "\toutput = activation(result)\n",
        "\n",
        "  # Find the error: how much this output differs from the expected output\n",
        "\terror = expected - output\n",
        "\n",
        "\t# Add the errors to the error list, so we can display it later\n",
        "\terrors.append(error)\n",
        "\n",
        "  # Adjust the weights\n",
        "\tweights += learning_rate * error * x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opq-B1qrQLJx"
      },
      "source": [
        "So, how did we do? Let's go through the examples in the training set, and fire our weighted perceptron - using the learning weights, $w$ - for each eaxmple."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Go through all of our data, and see what the perceptron\n",
        "# outputs for it, now it is trained\n",
        "for x, _ in training_data:\n",
        "  result = dot(x, weights)\n",
        "  output = activation(result)\n",
        "  print(\"{}: {} -> {}\".format(x[:2], result, output))\n"
      ],
      "metadata": {
        "id": "WnVLIgI2XVnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKp9daX4QLJy"
      },
      "source": [
        "How does it look? Did we nail it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRsVTeu3QLJy"
      },
      "source": [
        "Finally, let's print a graph of those errors, to see how the process went."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAZz0n17QLJy"
      },
      "outputs": [],
      "source": [
        "plt.ylim([-1,1])\n",
        "plt.plot(errors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbbhO8E9QLJy"
      },
      "source": [
        "### Exercises for you to try"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l9tnpS2QLJy"
      },
      "source": [
        "Try adapting the training data array to model these functions, running again for each one:\n",
        "* OR  - output is 1 if either or both inputs are 1\n",
        "* NAND -  output is 1 if and only if both inputs are 0 (not and)\n",
        "* XOR  -  ouput is 1 if only one of the inputs is 1, not if both or none are 1 (exclusive or)\n",
        "\n",
        "What did you find? Did they all work? If not, why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfcfmvPjQLJz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
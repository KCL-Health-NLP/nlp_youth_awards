{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCL-Health-NLP/nlp_youth_awards/blob/main/practicals/embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OjBWQtuFo0w"
      },
      "source": [
        "## Word embeddings\n",
        "*(Credit: Leon Derczynski, IT University of Copenhagen, extended and adapted by Angus Roberts)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jahAH9FcFo0z"
      },
      "source": [
        "We will create some embeddings from text *corpora* - collections of texts, and also load some pre-built ones. We will use these to see which words are close to each other, to get an idea of how embeddings work and how good they are.\n",
        "\n",
        "We'll import a few Python packages first. We will use the gensim package's word2vec implementation - word2vec is a popular embedding model. We will also use nltk, a popular natural language processing toolkit. We will oad a few tools from this:\n",
        "\n",
        "- Brown Corpus - a collection of 500 standard American English texts, each of roughly 2000 words.\n",
        "- Movie Reviews corpus - contains 1000 positive and 1000 negative movie reviews.\n",
        "- punkt - a *tokeniser* which will split texts in to their constituent words.\n",
        "\n",
        "We will also use the Python seaborn data visualisation package to visualise embeddings as heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will install gensim, a Python language modelling library\n",
        "# that includes some embedding model implementations\n",
        "!pip install --upgrade gensim\n",
        "\n",
        "# At the time of writing, there was a bug in gensim. It can\n",
        "# be worked round by restarting your colab runtime. We do it\n",
        "# in code here, using the os (operating system) package and the\n",
        "# its kill method.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "iR3K3Pmo68al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNhihIEDFo00"
      },
      "outputs": [],
      "source": [
        "# A word2vec implementation\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load some corpora\n",
        "from nltk.corpus import brown, movie_reviews\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "# A tokeniser\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# We'll use seaborn to visualise embeddings as heatmaps\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL-pGzhdFo04"
      },
      "source": [
        "Let's generate word vectors over the Brown corpus text. We will have 20 dimensions, using a window of three context words on each side (e.g. c-3, c-2, c-1, w, c+1, c+2, c+3). This might be a little slow (maybe 1-2 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFlJ2Tz0Fo05"
      },
      "outputs": [],
      "source": [
        "# Create embeddings for the Brown corpus\n",
        "b = Word2Vec(brown.sents(), vector_size=20, window=3, min_count=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOpnc7g9Fo07"
      },
      "source": [
        "Gensim's Word2Vec package has some useful methods to compare and manipulate embedding vectors. We will use one of these, *most_similar* to find words that are similar to each other, and test how good our Brown embedding is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQIw3mJvFo08"
      },
      "outputs": [],
      "source": [
        "# Find the five most similar embeddings to the one for a given word\n",
        "b.wv.most_similar('company', topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBSdnlU4Fo1C"
      },
      "source": [
        "Not great, eh? Try altering the window and the dimension size, to see if you get better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0b2OJTOFo1D"
      },
      "source": [
        "Try also with the movie reviews results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad2VWdpcFo1D"
      },
      "outputs": [],
      "source": [
        "# Build an embedding for the movie review corpus\n",
        "mr = Word2Vec(movie_reviews.sents(), vector_size=20, window=5, min_count=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01mNdhm8Fo1I"
      },
      "outputs": [],
      "source": [
        "# Find the five most similar embeddings to the one for a given word\n",
        "mr.wv.most_similar('love', topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTYsddU5Fo1L"
      },
      "source": [
        "We can also do some arithmetic with the word vectors. Let's try that classical result, king - man + woman."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjrmbgvCFo1M"
      },
      "outputs": [],
      "source": [
        "b.wv.most_similar(positive=['biggest', 'small'], negative=['big'], topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyFNDhcJFo1Q"
      },
      "source": [
        "Not a perfect result with the default model! Why don't we try loading a bigger dataset, based on a bigger vocabulary. This should give better results. Rather than build one from scratch, we will load an embedding that has already been trained, and saved to disk. You'll need the GloVe embeddings for this, which we will download from github."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy files from github in to the local Colab filespace.\n",
        "!git clone --quiet https://github.com/KCL-Health-NLP/nlp_youth_awards.git\n",
        "print(\"Done copying files\")"
      ],
      "metadata": {
        "id": "o0BnmUlVRVcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrQFNE0oyYSU"
      },
      "source": [
        "Now let's load the model file. This might take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4qKmtOuypp6"
      },
      "outputs": [],
      "source": [
        "# KeyedVectors can be used to implement the GloVe embeddings\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "# Load a pre-trained GloVe embedding from a compressed file\n",
        "glove = KeyedVectors.load_word2vec_format(\"nlp_youth_awards/practicals/glove.twitter.27B.25d.txt.bz2\", binary=False)\n",
        "print(\"Done loading\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26LIcnK9Fo1V"
      },
      "source": [
        "Now, try the above again. Can you find any cool word combinations? What differences are there in the datasets?\n",
        "\n",
        "Here are some ideas to try, substitute your own words in to these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0tsWpenFo1W"
      },
      "outputs": [],
      "source": [
        "glove.most_similar('meat', topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwhIzDHeFo1Z"
      },
      "outputs": [],
      "source": [
        "glove.most_similar(positive=['biggest', 'small'], negative=['big'], topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsnpDVSOFo1d"
      },
      "outputs": [],
      "source": [
        "glove.most_similar(positive=['woman', 'king'], negative=['man'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9xvpKT2Fo1i"
      },
      "outputs": [],
      "source": [
        "# Measures the similarity between two embeddings\n",
        "glove.similarity('car', 'bike')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzUGyMgpFo1l"
      },
      "outputs": [],
      "source": [
        "glove.similarity('car', 'purple')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv_i2NIyFo1p"
      },
      "outputs": [],
      "source": [
        "glove.similarity('red', 'purple')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNyv2H2NFo1t"
      },
      "outputs": [],
      "source": [
        "# Finds the least similar embedding in a list\n",
        "glove.doesnt_match(\"breakfast cereal dinner lunch\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPOmgAzXFo1x"
      },
      "outputs": [],
      "source": [
        "glove.doesnt_match(\"red green horse blue\".split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-vQBPAwadgI"
      },
      "source": [
        "What about ambiguous words? Can you think of any and try them? Past suggestions have been cancer, bank and play. Can you find any others, and explain what is going on? How does the embedding deal with ambiguity? What factors influence this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE8jPa8Wa-xk"
      },
      "outputs": [],
      "source": [
        "glove.most_similar('word')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMjn2ZIbXFqM"
      },
      "source": [
        "What do these embeddings look like? We will display embeddings for four words: two colour adjectives, and two action verbs. Each column is the embedding for one word. We have printed to two decimal places, using Python string formatting. Can you spot any similarities and differences?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNf_9myuFo12"
      },
      "outputs": [],
      "source": [
        "# Column headings\n",
        "print(\"   red      green             walk    run\\n\")\n",
        "\n",
        "# For each number i from 0 to the length of our embeddings\n",
        "for i in range(len(glove['red'])):\n",
        "\n",
        "  # Print the value of the four embeddings at this position\n",
        "  print(\"%8.2f%8.2f          %8.2f%8.2f\" % (glove['red'][i], glove['green'][i], glove['walk'][i], glove['run'][i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP2NRsrDc32F"
      },
      "source": [
        "Let's visualise this as a heatmap, using seaborn (imported as sns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiWymgpjc32F"
      },
      "outputs": [],
      "source": [
        "# Display a heatmap in which the value of the embedding at each position is\n",
        "# represented by a different colour intensity\n",
        "sns.heatmap([glove['red'], glove['green'], glove['walk'], glove['run']],\n",
        "            cmap = 'coolwarm', vmin = -2, vmax = 1.5,\n",
        "            yticklabels=['red', 'green', 'walk', 'run'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjo-IFo5X-54"
      },
      "source": [
        "How do we use these embeddings in NLP? The usual way is to replace each occurence of a word with an embedding - it represents our word. The example below displays what we would pass to our algorithm for a sentence. We show one line for each word, with each value formatted to two decimal places again. The word is displayed at the start of the line for convenience only - this would not be passed to our algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b-1ZJ9kP1uX"
      },
      "outputs": [],
      "source": [
        "# We will look at the embeddings for this sentence\n",
        "sentence=[\"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "\n",
        "# An empty list in to which we will put the embeddings before printing them\n",
        "embeddings = []\n",
        "\n",
        "# For each word in the sentence\n",
        "for w in sentence:\n",
        "  embeddings.append(glove[w])\n",
        "\n",
        "# For each embedding in the embeddings list, and it's position i\n",
        "for i, em in enumerate(embeddings):\n",
        "\n",
        "  # Print the word at index i, and the values (x) in it's embedding (em)\n",
        "  print(sentence[i].ljust(10), ''.join(\"{:6.2f}\".format(x) for x in em))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "8-embeddings.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}